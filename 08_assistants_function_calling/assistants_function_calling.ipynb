{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assistants API - Function Calling\n",
    "\n",
    "An assistant is a purpose-built AI that has specific instructions, leverages extra knowledge, and can call models and tools to perform tasks.\n",
    "\n",
    "https://platform.openai.com/docs/assistants/tools/function-calling\n",
    "\n",
    "https://cookbook.openai.com/examples/assistants_api_overview_python\n",
    "\n",
    "https://dev.to/esponges/build-the-new-openai-assistant-with-function-calling-52f5 \n",
    "\n",
    "https://community.openai.com/t/function-calling-with-assistants-api/488259/2 \n",
    "\n",
    "https://community.openai.com/t/function-calling-with-assistants-api/488259 \n",
    "\n",
    "https://dev.to/airtai/function-calling-and-code-interpretation-with-openais-assistant-api-a-quick-and-simple-tutorial-5ce5\n",
    "\n",
    "https://cobusgreyling.medium.com/what-are-openai-assistant-function-tools-exactly-06ef8e39b7bd\n",
    "\n",
    "Watch:\n",
    "\n",
    "https://www.youtube.com/watch?v=BV-_5_r46kE&t=0s\n",
    "\n",
    "https://www.youtube.com/watch?v=SaJxbuKehpc \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ : bool = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "client : OpenAI = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function calling\n",
    "\n",
    "Similar to the Chat Completions API, the Assistants API supports function calling. Function calling allows you to describe functions to the Assistants and have it intelligently return the functions that need to be called along with their arguments. The Assistants API will pause execution during a Run when it invokes functions, and you can supply the results of the function call back to continue the Run execution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Define functions\n",
    "First, define your functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def getCurrentWeather(location:str, unit:str=\"fahrenheit\")->str:\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    if \"tokyo\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"})\n",
    "    elif \"los angeles\" in location.lower():\n",
    "        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"})\n",
    "    elif \"paris\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"})\n",
    "    else:\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n",
    "    \n",
    "\n",
    "def getNickname(location:str)->str:\n",
    "    \"\"\"Get the nickname of a city\"\"\"\n",
    "    if \"tokyo\" in location.lower():\n",
    "        return \"tk\"\n",
    "    elif \"los angeles\" in location.lower():\n",
    "        return \"la\"\n",
    "    elif \"paris\" in location.lower():\n",
    "        return \"py\"\n",
    "    else:\n",
    "        return location\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create an Assistant and register/report your functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def show_json(message, obj):\n",
    "    display(message, json.loads(obj.model_dump_json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.beta import Assistant\n",
    "\n",
    "assistant: Assistant = client.beta.assistants.create(\n",
    "  instructions=\"You are a weather bot. Use the provided functions to answer questions.\",\n",
    "  model=\"gpt-3.5-turbo-1106\",\n",
    "  tools=[{\n",
    "      \"type\": \"function\",\n",
    "    \"function\": {\n",
    "      \"name\": \"getCurrentWeather\",\n",
    "      \"description\": \"Get the weather in location\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"location\": {\"type\": \"string\", \"description\": \"The city and state e.g. San Francisco, CA\"},\n",
    "          \"unit\": {\"type\": \"string\", \"enum\": [\"c\", \"f\"]}\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "      }\n",
    "    }\n",
    "  }, {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "      \"name\": \"getNickname\",\n",
    "      \"description\": \"Get the nickname of a city\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"location\": {\"type\": \"string\", \"description\": \"The city and state e.g. San Francisco, CA\"},\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "      }\n",
    "    } \n",
    "  }]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create a Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread(id='thread_ehCKTBispvP0qpRTJ8Ce1vzE', created_at=1700412426, metadata={}, object='thread')\n"
     ]
    }
   ],
   "source": [
    "from openai.types.beta.thread import Thread\n",
    "\n",
    "thread: Thread  = client.beta.threads.create()\n",
    "\n",
    "print(thread)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Add a Message to a Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.beta.threads.thread_message import ThreadMessage\n",
    "\n",
    "# First Request\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"How is the weather in Los Angles?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run the Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.beta.threads.run import Run\n",
    "\n",
    "run: Run = client.beta.threads.runs.create(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Life Cycle\n",
    "\n",
    "![Alt text](diagram.png \"run life cycle\")\n",
    "\n",
    "### STATUS\tDEFINITION\n",
    "\n",
    "https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps\n",
    "\n",
    "queued:\t\n",
    "\n",
    "When Runs are first created or when you complete the required_action, they are moved to a queued status. They should almost immediately move to in_progress.\n",
    "\n",
    "in_progress:\t\n",
    "\n",
    "While in_progress, the Assistant uses the model and tools to perform steps. You can view progress being made by the Run by examining the Run Steps.\n",
    "\n",
    "completed:\t\n",
    "\n",
    "The Run successfully completed! You can now view all Messages the Assistant added to the Thread, and all the steps the Run took. You can also continue the conversation by adding more user Messages to the Thread and creating another Run.\n",
    "\n",
    "requires_action:\t\n",
    "\n",
    "When using the Function calling tool, the Run will move to a required_action state once the model determines the names and arguments of the functions to be called. You must then run those functions and submit the outputs before the run proceeds. If the outputs are not provided before the expires_at timestamp passes (roughly 10 mins past creation), the run will move to an expired status.\n",
    "\n",
    "expired:\n",
    "\n",
    "This happens when the function calling outputs were not submitted before expires_at and the run expires. Additionally, if the runs take too long to execute and go beyond the time stated in expires_at, our systems will expire the run.\n",
    "\n",
    "cancelling:\t\n",
    "\n",
    "You can attempt to cancel an in_progress run using the Cancel Run endpoint. Once the attempt to cancel succeeds, status of the Run moves to cancelled. Cancellation is attempted but not guaranteed.\n",
    "cancelled\tRun was successfully cancelled.\n",
    "\n",
    "failed:\t\n",
    "\n",
    "You can view the reason for the failure by looking at the last_error object in the Run. The timestamp for the failure will be recorded under failed_at.\n",
    "\n",
    "### Polling for updates\n",
    "\n",
    "In order to keep the status of your run up to date, you will have to periodically retrieve the Run object. You can check the status of the run each time you retrieve the object to determine what your application should do next. We plan to add support for streaming to make this simpler in the near future.\n",
    "\n",
    "### Thread locks\n",
    "\n",
    "When a Run is in_progress and not in a terminal state, the Thread is locked. This means that:\n",
    "\n",
    "New Messages cannot be added to the Thread.\n",
    "\n",
    "New Runs cannot be created on the Thread.\n",
    "\n",
    "## Run steps\n",
    "\n",
    "![Alt text](diagram-2.png \"run steps\")\n",
    "\n",
    "Most of the interesting detail in the Run Step object lives in the step_details field. There can be two types of step details:\n",
    "\n",
    "1. message_creation: This Run Step is created when the Assistant creates a Message on the Thread.\n",
    "2. tool_calls: This Run Step is created when the Assistant calls a tool. Details around this are covered in the relevant sections of the Tools guide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_functions = {\n",
    "            \"getCurrentWeather\": getCurrentWeather,\n",
    "            \"getNickname\": getNickname\n",
    "        } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Polling for Updates and Calling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Run Steps:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'data': [],\n",
       " 'object': 'list',\n",
       " 'first_id': None,\n",
       " 'last_id': None,\n",
       " 'has_more': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run is queued. Waiting...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Run Steps:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': 'step_t3WVNLIKurhVROqXeouf1J1Y',\n",
       "   'assistant_id': 'asst_NxkcsvKRpq9y0V8ez6B36R6X',\n",
       "   'cancelled_at': None,\n",
       "   'completed_at': None,\n",
       "   'created_at': 1700412428,\n",
       "   'expired_at': None,\n",
       "   'failed_at': None,\n",
       "   'last_error': None,\n",
       "   'metadata': None,\n",
       "   'object': 'thread.run.step',\n",
       "   'run_id': 'run_5eLpfu0ebUUrO9U0s1h4AUse',\n",
       "   'status': 'in_progress',\n",
       "   'step_details': {'tool_calls': [{'id': 'call_GelDctJH7yDtXitceEZE3G44',\n",
       "      'function': {'arguments': '{\"location\":\"Los Angeles, CA\"}',\n",
       "       'name': 'getCurrentWeather',\n",
       "       'output': None},\n",
       "      'type': 'function'}],\n",
       "    'type': 'tool_calls'},\n",
       "   'thread_id': 'thread_ehCKTBispvP0qpRTJ8Ce1vzE',\n",
       "   'type': 'tool_calls',\n",
       "   'expires_at': 1700413026}],\n",
       " 'object': 'list',\n",
       " 'first_id': 'step_t3WVNLIKurhVROqXeouf1J1Y',\n",
       " 'last_id': 'step_t3WVNLIKurhVROqXeouf1J1Y',\n",
       " 'has_more': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status:  requires_action\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'submit_tool_outputs'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'submit_tool_outputs': {'tool_calls': [{'id': 'call_GelDctJH7yDtXitceEZE3G44',\n",
       "    'function': {'arguments': '{\"location\":\"Los Angeles, CA\"}',\n",
       "     'name': 'getCurrentWeather'},\n",
       "    'type': 'function'}]},\n",
       " 'type': 'submit_tool_outputs'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toolCalls present\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Expected tool outputs for call_ids ['call_GelDctJH7yDtXitceEZE3G44'], got []\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/ZiaKhan/Documents/GitHub/learn-generative-ai/08_assistants_function_calling/assistants_function_calling.ipynb Cell 19\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ZiaKhan/Documents/GitHub/learn-generative-ai/08_assistants_function_calling/assistants_function_calling.ipynb#X23sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m           tool_outputs\u001b[39m.\u001b[39mappend({\u001b[39m\"\u001b[39m\u001b[39mtool_call_id\u001b[39m\u001b[39m\"\u001b[39m: toolcall\u001b[39m.\u001b[39mid,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ZiaKhan/Documents/GitHub/learn-generative-ai/08_assistants_function_calling/assistants_function_calling.ipynb#X23sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m: response,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ZiaKhan/Documents/GitHub/learn-generative-ai/08_assistants_function_calling/assistants_function_calling.ipynb#X23sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m                 })\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ZiaKhan/Documents/GitHub/learn-generative-ai/08_assistants_function_calling/assistants_function_calling.ipynb#X23sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39m# Submit tool outputs and update the run\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ZiaKhan/Documents/GitHub/learn-generative-ai/08_assistants_function_calling/assistants_function_calling.ipynb#X23sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     client\u001b[39m.\u001b[39mbeta\u001b[39m.\u001b[39mthreads\u001b[39m.\u001b[39mruns\u001b[39m.\u001b[39msubmit_tool_outputs(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ZiaKhan/Documents/GitHub/learn-generative-ai/08_assistants_function_calling/assistants_function_calling.ipynb#X23sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m         thread_id\u001b[39m=\u001b[39mthread\u001b[39m.\u001b[39mid,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ZiaKhan/Documents/GitHub/learn-generative-ai/08_assistants_function_calling/assistants_function_calling.ipynb#X23sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m         run_id\u001b[39m=\u001b[39mrun\u001b[39m.\u001b[39mid,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ZiaKhan/Documents/GitHub/learn-generative-ai/08_assistants_function_calling/assistants_function_calling.ipynb#X23sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m         tool_outputs\u001b[39m=\u001b[39mtool_outputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ZiaKhan/Documents/GitHub/learn-generative-ai/08_assistants_function_calling/assistants_function_calling.ipynb#X23sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39melif\u001b[39;00m run\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcompleted\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ZiaKhan/Documents/GitHub/learn-generative-ai/08_assistants_function_calling/assistants_function_calling.ipynb#X23sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m   \u001b[39m# List the messages to get the response\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ZiaKhan/Documents/GitHub/learn-generative-ai/08_assistants_function_calling/assistants_function_calling.ipynb#X23sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m   messages: \u001b[39mlist\u001b[39m[ThreadMessage] \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mbeta\u001b[39m.\u001b[39mthreads\u001b[39m.\u001b[39mmessages\u001b[39m.\u001b[39mlist(thread_id\u001b[39m=\u001b[39mthread\u001b[39m.\u001b[39mid)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/resources/beta/threads/runs/runs.py:309\u001b[0m, in \u001b[0;36mRuns.submit_tool_outputs\u001b[0;34m(self, run_id, thread_id, tool_outputs, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39mWhen a run has the `status: \"requires_action\"` and `required_action.type` is\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39m`submit_tool_outputs`, this endpoint can be used to submit the outputs from the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    308\u001b[0m extra_headers \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mOpenAI-Beta\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39massistants=v1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(extra_headers \u001b[39mor\u001b[39;00m {})}\n\u001b[0;32m--> 309\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post(\n\u001b[1;32m    310\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/threads/\u001b[39m\u001b[39m{\u001b[39;00mthread_id\u001b[39m}\u001b[39;00m\u001b[39m/runs/\u001b[39m\u001b[39m{\u001b[39;00mrun_id\u001b[39m}\u001b[39;00m\u001b[39m/submit_tool_outputs\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    311\u001b[0m     body\u001b[39m=\u001b[39mmaybe_transform(\n\u001b[1;32m    312\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mtool_outputs\u001b[39m\u001b[39m\"\u001b[39m: tool_outputs}, run_submit_tool_outputs_params\u001b[39m.\u001b[39mRunSubmitToolOutputsParams\n\u001b[1;32m    313\u001b[0m     ),\n\u001b[1;32m    314\u001b[0m     options\u001b[39m=\u001b[39mmake_request_options(\n\u001b[1;32m    315\u001b[0m         extra_headers\u001b[39m=\u001b[39mextra_headers, extra_query\u001b[39m=\u001b[39mextra_query, extra_body\u001b[39m=\u001b[39mextra_body, timeout\u001b[39m=\u001b[39mtimeout\n\u001b[1;32m    316\u001b[0m     ),\n\u001b[1;32m    317\u001b[0m     cast_to\u001b[39m=\u001b[39mRun,\n\u001b[1;32m    318\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1055\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1042\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1043\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1051\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1052\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1053\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1054\u001b[0m     )\n\u001b[0;32m-> 1055\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest(cast_to, opts, stream\u001b[39m=\u001b[39mstream, stream_cls\u001b[39m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:834\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    826\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    827\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    832\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    833\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 834\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request(\n\u001b[1;32m    835\u001b[0m         cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m    836\u001b[0m         options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m    837\u001b[0m         stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m    838\u001b[0m         stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m    839\u001b[0m         remaining_retries\u001b[39m=\u001b[39mremaining_retries,\n\u001b[1;32m    840\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:877\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    875\u001b[0m     \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 877\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    879\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Expected tool outputs for call_ids ['call_GelDctJH7yDtXitceEZE3G44'], got []\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "  # Loop until the run completes or requires action\n",
    "while True:\n",
    "  runStatus = client.beta.threads.runs.retrieve(thread_id=thread.id,\n",
    "  run_id=run.id)\n",
    "\n",
    "  # Add run steps retrieval here for debuging\n",
    "  run_steps = client.beta.threads.runs.steps.list(thread_id=thread.id, run_id=run.id)\n",
    "  show_json(\"Run Steps:\", run_steps)\n",
    "\n",
    "  # This means run is making a function call   \n",
    "  if runStatus.status == \"requires_action\": \n",
    "    print(\"Status: \", \"requires_action\")\n",
    "    show_json(\"submit_tool_outputs\", runStatus.required_action)\n",
    "    if runStatus.required_action.submit_tool_outputs and runStatus.required_action.submit_tool_outputs.tool_calls:\n",
    "      print(\"toolCalls present\")\n",
    "      toolCalls = runStatus.required_action.submit_tool_outputs.tool_calls\n",
    "      tool_outputs = []\n",
    "      for toolcall in toolCalls:\n",
    "        function_name = toolcall.function.name\n",
    "        function_args = json.loads(toolcall.function.arguments)\n",
    "\n",
    "        if function_name in available_functions:\n",
    "          function_to_call = available_functions[function_name]\n",
    "        \n",
    "          if function_to_call == \"getCurrentWeather\":\n",
    "            response = function_to_call(\n",
    "              location=function_args.get(\"location\"),\n",
    "              unit=function_args.get(\"unit\"),\n",
    "            )\n",
    "            tool_outputs.append({\n",
    "                      \"tool_call_id\": toolcall.id,\n",
    "                      \"output\": output\n",
    "                  })\n",
    "          elif function_to_call == \"getNickname\":\n",
    "            response = function_to_call(\n",
    "              location=function_args.get(\"location\")\n",
    "            )\n",
    "            tool_outputs.append({\"tool_call_id\": toolcall.id,\n",
    "                      \"output\": response,\n",
    "                  })\n",
    "\n",
    "      # Submit tool outputs and update the run\n",
    "      client.beta.threads.runs.submit_tool_outputs(\n",
    "          thread_id=thread.id,\n",
    "          run_id=run.id,\n",
    "          tool_outputs=tool_outputs)\n",
    "    \n",
    "  elif run.status == \"completed\":\n",
    "    # List the messages to get the response\n",
    "    messages: list[ThreadMessage] = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "    for message in messages.data:\n",
    "        role_label = \"User\" if message.role == \"user\" else \"Assistant\"\n",
    "        message_content = message.content[0].text.value\n",
    "        print(f\"{role_label}: {message_content}\\n\")\n",
    "    break  # Exit the loop after processing the completed run\n",
    "\n",
    "  elif run.status == \"failed\":\n",
    "    print(\"Run failed.\")\n",
    "    break\n",
    "\n",
    "  elif run.status in [\"in_progress\", \"queued\"]:\n",
    "    print(f\"Run is {run.status}. Waiting...\")\n",
    "    time.sleep(5)  # Wait for 5 seconds before checking again\n",
    "\n",
    "  else:\n",
    "    print(f\"Unexpected status: {run.status}\")\n",
    "    break\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
